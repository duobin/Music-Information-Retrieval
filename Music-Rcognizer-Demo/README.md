# Music Recognizer Demo

```Python + SCiPy + SQLite```

This project implemented and evaluated a music recognizer. 



<img src="https://raw.githubusercontent.com/hyqshr/MD_picgo/main/22tOFYboXemakz5DpfmdHdFdmZy8nNAf6teuON2_2ArR9PAN86sdxCwz1TtotmPZdntG4i7SQuSFQumCeNOwvJ-RFVMawq9Ob2QOYe7I3CKzhMCJhki90-jlvUl8wMZvkUhm8c1oEguG4S-gBQ" alt="img" style="zoom:67%;" />

Credit to：

http://coding-geek.com/how-shazam-works/

https://willdrevo.com/fingerprinting-and-audio-recognition-with-python/

original Shazam paper:

https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf



## Installing

```
pip install numpy termcolor pyaudio wave pydub
```

## How to use

1. Run reset.py file to clear and re-initialize your database

```
python Initialize.py
```

2. Put your mp3 files into mp3 folder
3. Run analyze.py 

```
python Sync.py
```

4. When you see indexed musics to database and run Listen.py to listen music to discover. 

```
python Listen.py
```



For evaluation, you can record some query in ```evaluation/recorded_song``` and then run evaluate_with_origin_songs.py and evaluate_with_recorded_song.py, it will output ```result.csv``` file.

## How is it work

### Fast Fourier Transform

In short, FFT "map" the audio  representation from original amplitude-time field to frequency-amplitude-time field. It is just anthoer way to represent the audio.



### The basic idea

This is the idea for fingerprinting.

![img](https://raw.githubusercontent.com/hyqshr/MD_picgo/main/UhZMUPEBGJGW34gxJ28vXW_poRkaQMkRCxtBt0c9Q1cp1pxgNzCCaOF8TJbrK2XRsuCXHmsO8fNnOv63hKrouxtzdTpQmpuTCS-2zkBQI8-0Q-Co5eu7hYhYh6_Xe2lcmWU6ixhEUSMr3X_zdg)

 [Figure Source](https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf)

An audio fingerprint is generated from an audio signal that can be used to identify an audio sample. 

With the spectrogram generated by FFT(1A), I then use a max filter to iterate over the spectrogram image to make sure that only the peaks are kept (1B). 

In a specified number of “neighborhood” peaks, a hash will be generated between the peak and each of its neighborhood(1C). 

The hash is generated by both the peak’s amplitude and the time delta between the peaks(1D). 

### How query be processed?

- Input the audio clip. With pyaudio and a 44100 sample rate, you will have a list of number with around 438272. It looks like:

```
data = [0, 0, -1, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, -1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1...]
```

- Calculate the spectrum of the audio clip. It will become a matrix of shape [2049,213].

<img src="https://raw.githubusercontent.com/hyqshr/MD_picgo/main/image-20220426011730261.png" alt="image-20220426011730261" style="zoom:67%;" />

- It's the time for image processing. You need to find the amplitude peak in the spectrum and save the **peak and its time index**. How to do so? You can use a struct like this (always bigger than this)  with a *maximum_filter* function from scipy to iterate the image and only save the maximum point in its "neighborhood":

![image-20220426012514309](https://raw.githubusercontent.com/hyqshr/MD_picgo/main/image-20220426012514309.png)



After that, you will have list -> list((peaks_1,timeID_1),(peaks_2,timeID_2),(peaks_3,timeID_3)...)

*peaks = [(0, 0), (57, 13), (29, 16), (32, 41), (130, 41), (0, 48), (57, 93), (116, 93), (32, 113), (130, 113), (1, 121), (58, 125), (18, 127), (32, 137), (3, 148), (109, 151), (32, 167), (3, 202), (29, 206), (116, 206), (58, 207)]*

- Make hashes between each (peak,time) pair and the following #N of pairs , (#N is a hyper-parameter, lets say 15).  Each hash will look like:

```
{peak1}|{peak2}|{delta} 
#Here, delta is the difference between time
```

- Use hash function to process the string above, you can use library like hashlib in Python; and then each hash look like:

``` hash = 'abe01ef62919b88239a7'
hash = 'abe01ef62919b88239a7'
```

​	You will have a lot of hashes like this in from a 10 sec clips.

- Next, with those hashes, you will trying to match the hashes with all songs in the database and return the song with the most matching count.
